import React, { useEffect, useRef, useCallback } from "react";
import { YouTubeMetadata } from "../services/geminiService";
import { sonicEngine } from "../services/proceduralAudio";

interface RecordingSystemProps {
  isRecording: boolean;
  getCanvas: () => HTMLCanvasElement | null;
  audioRef: React.RefObject<HTMLAudioElement | null>;
  metadata: YouTubeMetadata | null;
  durationMinutes: number;
  onRecordingComplete: (blob: Blob) => void;
}

const RecordingSystem: React.FC<RecordingSystemProps> = ({
  isRecording,
  getCanvas,
  audioRef,
  metadata,
  durationMinutes,
  onRecordingComplete,
}) => {
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const sourceNodeRef = useRef<MediaElementAudioSourceNode | null>(null);
  const streamDestRef = useRef<MediaStreamAudioDestinationNode | null>(null);
  const musicGainRef = useRef<GainNode | null>(null);
  const currentMimeType = useRef<string>("");
  const recordingStartTimeRef = useRef<number>(0);
  const isRecordingActiveRef = useRef<boolean>(false);

  const initAudioGraph = useCallback(() => {
    const audioEl = audioRef.current;
    if (!audioEl) return null;

    const ctx = sonicEngine.getContext();
    if (!ctx) return null;

    try {
      // CRITICAL FIX: Check if audio element has a valid source before creating source node
      if (!audioEl.src && !audioEl.currentSrc) {
        console.warn(`âš ï¸ [AudioGraph] No audio source available, skipping audio graph setup`);
        // Return empty audio stream instead of null to avoid breaking the recorder
        const emptyDest = ctx.createMediaStreamDestination();
        return emptyDest.stream;
      }

      // Ø§ÛŒØ¬Ø§Ø¯ Ø³ÙˆØ±Ø³ ØµÙˆØªÛŒ (ÙÙ‚Ø· ÛŒÚ©Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù„Ù…Ù†Øª Ø¯Ø± Ø·ÙˆÙ„ Ú†Ø±Ø®Ù‡ Ø­ÛŒØ§Øª)
      if (!sourceNodeRef.current) {
        console.log(`ğŸ”Š [AudioGraph] Creating MediaElementAudioSourceNode...`);
        try {
          sourceNodeRef.current = ctx.createMediaElementSource(audioEl);
          console.log(`âœ… [AudioGraph] Source node created successfully`);
        } catch (e) {
          console.error(`âŒ [AudioGraph] Failed to create source node:`, e);
          // If we can't create source node, return empty stream
          const emptyDest = ctx.createMediaStreamDestination();
          return emptyDest.stream;
        }
      }

      // Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‚ØµØ¯ Ø§Ø³ØªØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø¶Ø¨Ø·
      if (!streamDestRef.current) {
        streamDestRef.current = ctx.createMediaStreamDestination();
      }

      // Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ØªØ±Ù„Ø± ÙˆÙ„ÙˆÙ… (Gain)
      if (!musicGainRef.current) {
        musicGainRef.current = ctx.createGain();
      }

      const musicSource = sourceNodeRef.current;
      const musicGain = musicGainRef.current;
      const dest = streamDestRef.current;

      // Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø§ÛŒÙ…Ù† (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Not Connected)
      try {
        musicSource.disconnect();
      } catch (e) {
        // Ø§Ú¯Ø± Ù…ØªØµÙ„ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
      }

      // Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ Ø§ØªØµØ§Ù„Ø§Øª Ú¯Ø±Ø§Ù ØµÙˆØªÛŒ
      musicSource.connect(musicGain);
      musicGain.connect(ctx.destination); // Ø¨Ø±Ø§ÛŒ Ø´Ù†ÛŒØ¯Ù† ØµØ¯Ø§ Ø§Ø² Ø§Ø³Ù¾ÛŒÚ©Ø±
      musicGain.connect(dest); // Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ø±ÛŒÚ©ÙˆØ±Ø¯Ø±

      console.log(`ğŸµ [AudioGraph] Audio routing complete: audio â†’ gain â†’ [speakers + recorder]`);

      // Ø§ØªØµØ§Ù„ Ø§ÙÚ©Øªâ€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ Ø³ÛŒØ³ØªÙ… (SFX) Ø¨Ù‡ Ø¶Ø¨Ø·
      const sfxGain = sonicEngine.getMasterGain();
      if (sfxGain) {
        try {
          sfxGain.disconnect(dest);
        } catch (e) {}
        sfxGain.connect(dest);
        console.log(`ğŸ”Š [AudioGraph] SFX routing complete: sfx â†’ recorder`);
      }

      return dest.stream;
    } catch (e) {
      console.error("Critical Audio Graph Error:", e);
      // Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ØŒ Ø­Ø¯Ø§Ù‚Ù„ Ø§Ø³ØªØ±ÛŒÙ… Ù…Ù‚ØµØ¯ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ… ØªØ§ Ø±ÛŒÚ©ÙˆØ±Ø¯ Ú©Ù„Ø§ Ù…ØªÙˆÙ‚Ù Ù†Ø´ÙˆØ¯
      return streamDestRef.current?.stream || null;
    }
  }, [audioRef]);

  // Reset audio source node when audio source changes (important for Auto Mode with different tracks)
  useEffect(() => {
    const audioEl = audioRef.current;
    if (!audioEl) return;

    const handleLoadStart = () => {
      console.log(`ğŸ”„ [RecordingSystem] Audio source changing, resetting source node...`);
      // Disconnect old source node safely
      if (sourceNodeRef.current) {
        try {
          sourceNodeRef.current.disconnect();
        } catch (e) {
          // Ignore disconnect errors
        }
        sourceNodeRef.current = null;
        console.log(`âœ… [RecordingSystem] Source node reset complete`);
      }
    };

    audioEl.addEventListener('loadstart', handleLoadStart);
    return () => {
      audioEl.removeEventListener('loadstart', handleLoadStart);
    };
  }, [audioRef]);

  // Define stopRecording BEFORE the useEffect that uses it
  const stopRecording = useCallback(() => {
    console.log(`ğŸ›‘ [RecordingSystem] Stop recording requested...`);
    console.log(`   MediaRecorder state: ${mediaRecorderRef.current?.state || 'null'}`);
    console.log(`   isRecordingActive: ${isRecordingActiveRef.current}`);

    if (!mediaRecorderRef.current || mediaRecorderRef.current.state === "inactive") {
      console.warn(`âš ï¸ [RecordingSystem] No active recording to stop`);
      isRecordingActiveRef.current = false;
      return;
    }

    const ctx = sonicEngine.getContext();
    const recordingDuration = (Date.now() - recordingStartTimeRef.current) / 1000;

    console.log(`   Recording duration so far: ${recordingDuration.toFixed(1)}s`);
    console.log(`   Chunks collected so far: ${chunksRef.current.length}`);

    // ÙÛŒØ¯-Ø§ÙˆØª Ø³Ø±ÛŒØ¹ ØµØ¯Ø§ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ ÙˆÛŒØ¯Ø¦Ùˆ
    if (musicGainRef.current && ctx) {
      try {
        musicGainRef.current.gain.linearRampToValueAtTime(0, ctx.currentTime + 0.5);
      } catch (e) {
        console.warn(`âš ï¸ Audio fade-out failed:`, e);
      }
    }

    // CRITICAL: Request final data chunk before stopping
    if (mediaRecorderRef.current.state === "recording") {
      console.log(`   ğŸ“¦ Requesting final data chunk...`);
      try {
        mediaRecorderRef.current.requestData();
      } catch (e) {
        console.warn(`âš ï¸ requestData failed:`, e);
      }
    }

    // ØªÙˆÙ‚Ù Ø±ÛŒÚ©ÙˆØ±Ø¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² Ù†ÛŒÙ… Ø«Ø§Ù†ÛŒÙ‡ ÙÛŒØ¯-Ø§ÙˆØª
    setTimeout(() => {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
        console.log(`   ğŸ›‘ Stopping MediaRecorder...`);
        mediaRecorderRef.current.stop();
      }
    }, 500);
  }, []);

  // startRecording function
  const startRecording = useCallback(async () => {
    const canvas = getCanvas();
    const audioEl = audioRef.current;

    console.log(`ğŸ¬ [RecordingSystem] Starting recording IMMEDIATELY...`);
    console.log(`   Canvas: ${canvas ? "OK" : "MISSING"}`);
    console.log(`   Audio Element: ${audioEl ? "OK" : "MISSING"}`);

    if (!canvas) {
      console.error(`âŒ [RecordingSystem] Cannot start - missing canvas`);
      return;
    }

    // Prevent double-start
    if (isRecordingActiveRef.current) {
      console.warn(`âš ï¸ [RecordingSystem] Recording already active, ignoring start request`);
      return;
    }

    isRecordingActiveRef.current = true;
    recordingStartTimeRef.current = Date.now();

    try {
      const ctx = sonicEngine.getContext();

      // Resume audio context immediately (non-blocking)
      if (ctx && ctx.state === "suspended") {
        console.log(`   Resuming suspended audio context...`);
        ctx.resume().catch(e => console.warn(`Audio context resume failed:`, e));
      }

      // CRITICAL FIX: Start video capture IMMEDIATELY - don't wait for audio
      console.log(`   ğŸ¥ Starting video capture IMMEDIATELY...`);
      const videoStream = (canvas as any).captureStream(60);
      const tracks = [...videoStream.getVideoTracks()];

      console.log(`   Video tracks captured: ${tracks.length}`);

      // Initialize audio graph synchronously (best effort)
      let audioStream: MediaStream | null = null;
      try {
        audioStream = initAudioGraph();
        if (audioStream) {
          const audioTracks = audioStream.getAudioTracks();
          console.log(`   Audio stream tracks: ${audioTracks.length}`);
          if (audioTracks.length > 0) {
            tracks.push(audioTracks[0]);
          }
        }
      } catch (e) {
        console.warn(`âš ï¸ [RecordingSystem] Audio graph init failed, continuing video-only:`, e);
      }

      // Start audio playback (non-blocking)
      if (audioEl && (audioEl.src || audioEl.currentSrc)) {
        console.log(`   ğŸµ Starting audio playback...`);
        audioEl.play().catch(e => {
          console.warn(`âš ï¸ Audio playback failed:`, e);
        });

        // Fade in audio
        if (musicGainRef.current && ctx) {
          musicGainRef.current.gain.setValueAtTime(0, ctx.currentTime);
          musicGainRef.current.gain.linearRampToValueAtTime(0.8, ctx.currentTime + 1.0);
        }
      } else {
        console.warn(`   âš ï¸ No audio source - recording video only`);
      }

      const combinedStream = new MediaStream(tracks);

      const mimeType =
        ["video/mp4;codecs=avc1", "video/webm;codecs=vp9", "video/webm"].find((t) =>
          MediaRecorder.isTypeSupported(t)
        ) || "video/webm";
      currentMimeType.current = mimeType;

      console.log(`   ğŸ“¼ Creating MediaRecorder with MIME: ${mimeType}`);

      const recorder = new MediaRecorder(combinedStream, {
        mimeType,
        videoBitsPerSecond: 25000000,
      });

      chunksRef.current = [];

      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
          console.log(`   ğŸ“¦ Chunk received: ${(e.data.size / 1024).toFixed(1)}KB (total chunks: ${chunksRef.current.length})`);
        }
      };

      recorder.onstop = () => {
        isRecordingActiveRef.current = false;
        const recordingDuration = (Date.now() - recordingStartTimeRef.current) / 1000;
        const finalBlob = new Blob(chunksRef.current, { type: currentMimeType.current });

        console.log(`\nğŸ“¹ [RecordingSystem] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`);
        console.log(`   Recording COMPLETE!`);
        console.log(`   Duration: ${recordingDuration.toFixed(1)}s`);
        console.log(`   Blob size: ${(finalBlob.size / 1024 / 1024).toFixed(2)}MB`);
        console.log(`   Chunks collected: ${chunksRef.current.length}`);
        console.log(`   MIME type: ${currentMimeType.current}`);
        console.log(`   Expected duration: ${durationMinutes * 60}s`);
        console.log(`â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n`);

        // CRITICAL: Verify blob is valid and complete
        if (finalBlob.size === 0) {
          console.error(`âŒ [RecordingSystem] FATAL: Blob is empty! No data was recorded!`);
        } else if (recordingDuration < (durationMinutes * 60 * 0.8)) {
          console.warn(`âš ï¸ [RecordingSystem] WARNING: Recording duration (${recordingDuration.toFixed(1)}s) is less than 80% of expected (${(durationMinutes * 60).toFixed(1)}s)`);
        } else {
          console.log(`âœ… [RecordingSystem] Recording duration looks good!`);
        }

        onRecordingComplete(finalBlob);
        console.log(`âœ… [RecordingSystem] onRecordingComplete callback invoked`);
      };

      recorder.onerror = (e) => {
        console.error(`âŒ [RecordingSystem] MediaRecorder error:`, e);
        isRecordingActiveRef.current = false;
      };

      // CRITICAL: Start recording IMMEDIATELY with smaller chunks for more reliable capture
      recorder.start(500); // 500ms chunks for finer granularity
      mediaRecorderRef.current = recorder;

      console.log(`   âœ… MediaRecorder started! Recording in progress...`);
      console.log(`   â±ï¸ Recording started at: ${new Date().toISOString()}`);

    } catch (e) {
      console.error("âŒ Recording Engine Failure:", e);
      isRecordingActiveRef.current = false;
    }
  }, [getCanvas, audioRef, durationMinutes, onRecordingComplete, initAudioGraph]);

  // Main effect to control recording
  useEffect(() => {
    console.log(`ğŸ”„ [RecordingSystem] isRecording changed to: ${isRecording}`);
    if (isRecording) {
      startRecording();
    } else {
      stopRecording();
    }

    // Cleanup on unmount
    return () => {
      if (isRecordingActiveRef.current) {
        console.log(`ğŸ§¹ [RecordingSystem] Cleanup: stopping active recording`);
        stopRecording();
      }
    };
  }, [isRecording, startRecording, stopRecording]);

  return null;
};

export default RecordingSystem;
